{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll open the various docs and see how big they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/training_set_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"training_set_values\" has 59,400 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"training_set_labels\" just tells you what the status of those 59,400 records is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/test_set_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"test_set_values\" is just like training_set_values, with fewer records (14,850)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/SubmissionFormat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"SubmissionFormat\" is like training_set_labels except the contestant/data scientist has to provide the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of available files\n",
    "There is a training set consisting of 59,400 records, a set of labels for those 59,400 records,  a test set consisting of 14,850 records, and a template for submitting labels for those 14,850 test records to an online contest.\n",
    "\n",
    "Because we do not have labels for the 14,850 records in the test set, those are useless to us. We will have to carve out training and test sets from the 59,400 records with known labels.\n",
    "\n",
    "# Examining the training set\n",
    "\n",
    "Let's look closer at the training set. First we'll drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 40)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the training set\n",
    "df = pd.read_csv('../data/training_set_values.csv')\n",
    "# check for duplicates, excluding the ids\n",
    "df[df.duplicated(subset=df.columns.difference(['id']))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicate records\n",
    "df.drop(df[df.duplicated(subset=df.columns.difference(['id']))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "funder                3635\n",
       "installer             3655\n",
       "subvillage             371\n",
       "public_meeting        3314\n",
       "scheme_management     3877\n",
       "scheme_name          28138\n",
       "permit                3056\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show columns with missing values and the number of values missing\n",
    "df[df.columns[df.isna().any()]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half the records are missing the 'scheme_name', so we'll drop that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59363, 39)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop scheme_name\n",
    "df.drop(columns='scheme_name', inplace=True)\n",
    "# show rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "funder                3635\n",
       "installer             3655\n",
       "subvillage             371\n",
       "public_meeting        3314\n",
       "scheme_management     3877\n",
       "scheme_name          28138\n",
       "permit                3056\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show columns with missing values and the number of values missing\n",
    "df[df.columns[df.isna().any()]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to the question of what to do with these missing values after we explore more of the data, i.e. the \"fictional zeros\" from the numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way to optimize which records to drop and which features to drop? Cycle through all combinations of features and see which way we lose the least information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['funder',\n",
       " 'installer',\n",
       " 'subvillage',\n",
       " 'public_meeting',\n",
       " 'scheme_management',\n",
       " 'permit']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_features = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dropna().shape[0] < df.shape[0]:\n",
    "        na_features.append(col)\n",
    "\n",
    "na_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 ['funder', 'installer', 'public_meeting', 'scheme_management', 'permit']\n",
      "0.06 ['funder', 'installer', 'public_meeting', 'scheme_management']\n",
      "0.07 ['public_meeting', 'scheme_management', 'permit']\n",
      "0.08 ['public_meeting', 'scheme_management']\n",
      "0.13 ['scheme_management']\n",
      "0.19 []\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(na_features)):\n",
    "\n",
    "    sub_list = list(combinations(na_features, n+1))\n",
    "\n",
    "    high_score = 0\n",
    "    high_score_features = []\n",
    "\n",
    "    for item in sub_list:\n",
    "        features = list(item)\n",
    "        score = df[features].dropna().shape[0]\n",
    "        if score > high_score:\n",
    "            high_score = score\n",
    "            high_score_features = item\n",
    "    high_score = round(1 - high_score / df.shape[0], 2)\n",
    "    result = []\n",
    "    for element in na_features:\n",
    "        if element not in list(high_score_features):\n",
    "            result.append(element)\n",
    "    print(high_score, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38838"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.permit.dropna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many unique values each of these columns has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[df.isna().any()]].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****For now\n",
    "We will drop these columns and move forward with the ones that are intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at just numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['number']).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None appear to be missing values, but this can be deceiving. Let's look closer for zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['number']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show value counts for amount_tsh feature\n",
    "df.amount_tsh.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"amount of water available to a water point\". *Most* of these values are zero. This seems like a very relevant feature, and it would be a shame if the zeros were some kind of error. Let's optimistically assume the zeros are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.gps_height.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is altitude (or elevation), in meters. There are probably too many zeros here for altitude to be real. We'll investigate this further after we look at longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show value counts for longitude feature\n",
    "df.longitude.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show value counts for latitude feature\n",
    "df.latitude.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few zeros or near-zeros for longitude and latitude. Let's first look at the 1,776 records that lack positional coordinates and see whether they're worth repairing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the numerical features of records with zero longitude\n",
    "df.select_dtypes(include=['number'])[df['longitude'] == 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These records (now 1,776 of them after dropping duplicates) seem worthless, as they all have probably-erroneous zero values for most of the other numerical features. The only numerical data they offer are their region and district codes. Let's find out where those regions and districts are before we drop the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show value counts for the region_code of the zero-longitude records\n",
    "df[df['longitude'] == 0]['region_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show value counts for the district_code of the zero-longitude records\n",
    "df[df['longitude'] == 0]['district_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe if we plot the positions of records *with* GPS coordinates from these districts, we'll get some idea of where these problematic records are coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate a geographical map of all listings in the districts where the 1,776 records lacking long/lat are\n",
    "fig, ax = plt.subplots(figsize=(11,8))\n",
    "df[(df['longitude'] != 0) & (df['district_code'].isin([1,2,4,6]))].plot.scatter(\n",
    "    x='longitude', y='latitude', c='district_code', cmap='Blues', ax=ax)\n",
    "fig.suptitle('Distribution of Districts with Missing GPS Coordinates', size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the 1,776 problematic records all come from the same handful of districts, the map offers no other clues about what they may have in common, because it appears that \"districts\" and \"regions\" are not essentially contiguous but rather each consist of several discrete clusters. In any case, it looks like it won't be possible to recover or even approximate the GPS data for these records, so we might as well drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all zero-longitude records\n",
    "df.drop(df[df['longitude'] == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make a color-coded plot to see whether the gps height data makes sense by comparing it to an available topographical map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure with two axes over two columns\n",
    "fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize=(20,8))\n",
    "# plot long/lat for nonzero longs with color gradient for elevation\n",
    "df.plot.scatter(x='longitude', y='latitude', c='gps_height', cmap='plasma', ax=ax1)\n",
    "# upload an image\n",
    "im = plt.imread(\"../images/topo_map.jpeg\")\n",
    "# display the image\n",
    "im = ax2.imshow(im)\n",
    "# hide X and Y axes label marks\n",
    "ax2.xaxis.set_tick_params(labelbottom=False)\n",
    "ax2.yaxis.set_tick_params(labelleft=False)\n",
    "# hide X and Y axes tick marks\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "# title\n",
    "fig.suptitle('Topographical Map Comparison', size=18)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense for the locations along the ocean to have zero elevation, but there are at least three inland clusters that seem more like they are just lacking elevation data. One solution would be to set elevation values equal to the median for all records that have matching geographical location features such as \"subvillage\", but it appears that this wouldn't help because the missing values are geographically set apart from non-missing values.\n",
    "\n",
    "# *what to do about this?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show value counts for num_private feature\n",
    "df.num_private.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.num_private > 0].num_private.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.region_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.district_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.population.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.population < 10].population.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.construction_year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first-glance summary of the numerical features\n",
    "\n",
    "None of the numerical features are missing any values, but some have a suspicious amount of zeros.\n",
    "\n",
    "* amount_tsh: \"Total Static Head\". Some kind of measure of available water. This is mostly zeroes.\n",
    "* gps_height: From a spot check, this appears to be given in meters. It has a lot of zeros.\n",
    "* longitude, latitude: 1,812 records are essentially (0,0).\n",
    "* num_private: It is not at all clear what this means.\n",
    "* region_code, district_code: There are fewer unique district codes, suggesting they are possibly broader? There are some district codes equal to zero, which may or may not be an error.\n",
    "* population: There are many zeroes, but perhaps this means they're just rural?\n",
    "* construction_year: There are many zeroes, which will need to be dealt with.\n",
    "\n",
    "First we'll check for duplicate records, excluding the ids, and drop any duplicates we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[df.isna().any()]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upshot of this initial exploration is that we should drop the zero-longitude records and at least the scheme_name column and possibly the other columns with missing values. Then we still have to deal with the erroneous zero values for at least elevation, population, and construction year.\n",
    "\n",
    "Then we may need to detect other problem values in the data in other ways that we can't see yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
